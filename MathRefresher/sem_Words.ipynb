{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superkisa/MaGaML/blob/main/MathRefresher/sem_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAmUD1eZHryf"
      },
      "source": [
        "# **Word vectors**\n",
        "\n",
        "\n",
        "In the previous exercise we observed that colors that we think of as similar are 'closer' to each other in RGB vector space. Is it possible to create a vector space for all English words that has this same 'closer in space is closer in meaning' property?\n",
        "\n",
        "The answer is yes! Luckily, you don't need to create those vectors from scratch. Many researchers have made downloadable databases of pre-trained vectors. One such project is [Stanford's Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). \n",
        "\n",
        "These $300$-dimensional vectors are included with $\\texttt{spaCy}$, and they're the vectors we'll be using in this exercise.\n",
        "\n",
        "![cosine similarity: picture](https://d33wubrfki0l68.cloudfront.net/d2742976a92aa4d6c39f19c747ec5f56ed1cec30/3803f/images/guide-to-word-vectors-with-gensim-and-keras_files/word2vec-king-queen-vectors.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymHr8XZIHsML"
      },
      "source": [
        "# The following will download the language model.\n",
        "# Resart the runtime (Runtime -> Restart runtime) after running this cell\n",
        "# (and don't run it for the second time).\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7yqHuGJ6e5"
      },
      "source": [
        "Let's load the model now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-8rsSkSBU8C"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNf5FAkm3Ljj"
      },
      "source": [
        "## **Word vectors: the first glance**\n",
        "\n",
        "You can see the vector of any word in $\\texttt{spaCy}$' s vocabulary using the $\\texttt{vector}$ attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx-IzWxQAgNN"
      },
      "source": [
        "# A 300-dimensional vector\n",
        "len(nlp('dog').vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8UP3QrxKZPG"
      },
      "source": [
        "nlp('dog').vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHWCqKcl55TY"
      },
      "source": [
        "## **Cosine similarity**\n",
        "\n",
        "**Cosine similarity** is a common way of assessing similarity between words in NLP. It is essentially defined as the cosine of the angle between the vectors representing the words of interest.\n",
        "\n",
        "Recall that the angle $\\phi$ between two non-zero vectors $u$ and $v$ can be computed as follows:\n",
        "\n",
        "$cos(\\phi) = \\frac{(u,v)}{||u||\\cdot||v||}$\n",
        "\n",
        "![](https://miro.medium.com/max/1394/1*_Bf9goaALQrS_0XkBozEiQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoi6FvPgMWid"
      },
      "source": [
        "Define a function computing cosine similarity between two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpJS01dmvGbe"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine(v1, v2):\n",
        "  # Your code here\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEwtAXJRMc-s"
      },
      "source": [
        "Test your function by computing similarities of some random pairs of words, e.g. $dog$ and $puppy$ vs. $dog$ and $kitten$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBooDbGvYOG"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgHDwfx8Mu66"
      },
      "source": [
        "## **Loading the text**\n",
        "\n",
        "Let's load the full text of *Alice in Wonderland*. It will serve us as a corpus of English words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am8NoIl2zMXi"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Alice in Wonderland\n",
        "response = requests.get('https://www.gutenberg.org/files/11/11-0.txt')\n",
        "\n",
        "# If you prefer Dracula, load this instead:\n",
        "#response = requests.get('https://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
        "\n",
        "# Extracting separate words from the text\n",
        "doc = nlp(response.text)\n",
        "tokens = list(set([w.text for w in doc if w.is_alpha]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAwABf4nNNR3"
      },
      "source": [
        "Check out the content of $\\texttt{tokens}$ now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4B4FRR6NRzx"
      },
      "source": [
        "# tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GfkThRpNUKL"
      },
      "source": [
        "Define a function that takes a word and lists the $n$ most similar words in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_h-7n50kia"
      },
      "source": [
        "def spacy_closest(tokens, new_vec, n=10):\n",
        "  # Your code here\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTLEiz9UNjrO"
      },
      "source": [
        "Try to find words similar to some random words, e.g. $good$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1JL2VrF0ltD"
      },
      "source": [
        "spacy_closest(tokens, nlp('good').vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZhjqSgNqNd"
      },
      "source": [
        "You can also get creative and search for combinations of words. For example, what is similar to $king - man + woman$? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKI4SrhMN-EV"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpD73pj8OGGt"
      },
      "source": [
        "## **Sentence vectors**\n",
        "\n",
        "We can also construct a vector representation for the whole sentence. For example, we can define it as an *average* of the   vectors representing the words in it.\n",
        "\n",
        "Let's take a random sentence *My favorite food is strawberry ice cream* and construct its vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5xr_3MkEPeM"
      },
      "source": [
        "sent = nlp('My favorite food is strawberry ice cream.')\n",
        "\n",
        "# Your code here\n",
        "# sentv ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMf_OllyOvfX"
      },
      "source": [
        "Let's also extract sentences (as opposed to individual words) from our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jazUz0WvDsa3"
      },
      "source": [
        "sents = list(doc.sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzDdce7xO7QZ"
      },
      "source": [
        "Define a function that takes a random sentence and lists $n$ most similar sentences from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ7pQe8xD1x0"
      },
      "source": [
        "def spacy_closest_sent(sentences, input_vec, n=10):\n",
        "  # Your code here\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6m06T18PDQ8"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkDCEWWwEzIc"
      },
      "source": [
        "for s in spacy_closest_sent(sents, sentv, n=10):\n",
        "  print(s)\n",
        "  print('\\n---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VewB5XqkPLdx"
      },
      "source": [
        "## **References**\n",
        "\n",
        "This notebook is inspired by a [tutorial by Allison Parrish](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)."
      ]
    }
  ]
}